# -*- coding: utf-8 -*-
"""Uber NY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W4mMWDw7951SQBpYSsTFF6_1Zad-mfgO
"""

'''
1 ) "uber-raw-data-janjune-15.csv" ->> this data contains all the entries/pickups from 'January' to 'June'
    Quite huge dataset having approx 15M data pts , so lets consider its sample which have approx 1M


2 ) "uber-raw-data-janjune-15_sample.csv" ->> this data is a sample of "uber-raw-data-janjune-15.csv"
    'Since above data is quite huge ~15 Million data pts , hence it is good to work with some sample
     if u do not have good specifications in your systems

'''



"""# 1.. Lets Read data for Analysis"""

### lets import all the necessary packages !

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import os

from google.colab import files
uploaded = files.upload()

import zipfile
import os

with zipfile.ZipFile("Datasets-20250820T225209Z-1-001.zip", 'r') as zip_ref:
    zip_ref.extractall("uber_data")  # You can choose another name or path here

# Optional: list contents
os.listdir("uber_data")

import os

files = os.listdir("uber_data/Datasets")
print(files)

import pandas as pd

df = pd.read_csv("uber_data/Datasets/uber-raw-data-janjune-15_sample.csv")
print(df.head())

uber_15 = pd.read_csv(r"uber_data/Datasets/uber-raw-data-janjune-15_sample.csv")

uber_15.shape

"""## 2.. Lets Perform Data pre-processing/Data cleaning !
        check data-type , check missing values , check whether duplicated values or not !
        ie Prepare Data for Analysis !
"""

type(uber_15)

uber_15.duplicated()

uber_15.duplicated().sum()

uber_15.drop_duplicates(inplace=True)

uber_15.duplicated().sum()

uber_15.shape

uber_15.dtypes

uber_15.isnull().sum()

uber_15['Pickup_date'][0]

type(uber_15['Pickup_date'][0])

uber_15['Pickup_date'] = pd.to_datetime(uber_15['Pickup_date'])

uber_15['Pickup_date'].dtype

uber_15['Pickup_date'][0]

type(uber_15['Pickup_date'][0])

uber_15.dtypes

'''
datetime64[ns] is a general dtype, while <M8[ns] is a specific dtype , ns is basicaly nano second..
Both are similar , it entirely how your numpy was compiled..

If u want to cross check using Code :
np.dtype('datetime64[ns]') == np.dtype('<M8[ns]')



'''

'''
Categorical data has : Object & bool data-types
Numerical data have : Integer & Float data-type


Categorical data refers to a data type that can be stored into groups/categories/labels
Examples of categorical variables are  age group, blood type etc..


Numerical data refers to the data that is in the form of numbers,
Examples of numerical data are height, weight, age etc..

Numerical data has two categories: discrete data and continuous data


Discrete data : It basically takes countable numbers like 1, 2, 3, 4, 5, and so on.
                age of a fly : 8 , 9 day etc..

Continuous data : which is continuous in nature
                  amount of sugar , 11.2 kg  , temp of a city  , your bank balance !



'''

'''

Variations of int are : ('int64','int32','int16') in numpy library..


Int16 is a 16 bit signed integer , it means it can store both positive & negative values
int16 has has a range of  (2^15 − 1) to -2^15
int16 has a length of 16 bits (2 bytes).. ie Int16 uses 16 bits


Int32 is a 32 bit signed integer , it means it storesboth positive & negative values
int32 has has a range of (2³¹ − 1) to  -2^31
int32 has a length of 32 bits (4 bytes),, ie Int32 uses 32 bits


Int64 is a 64 bit signed integer , it means it can store both positive & negative values
int64 has has a range of  (2^63 − 1) to -2^63
int64 has a length of 64 bits (8 bytes) , ie Int64 uses 64 bits.


The only difference is that int64 has max range of storing numbers , then comes int32 , then 16 , then int8

That means that Int64’s take up twice as much memory-and doing
operations on them may be a lot slower in some machine architectures.

However, Int64’s can represent numbers much more accurately than
32 bit floats.They also allow much larger numbers to be stored..

'''

'''

Variations of unsigned integer are : ('uint64','uint32','uint16','uint8') in numpy library..
By the way , all the variations of signed integers comes sub-class numpy.unsignedinteger

uint8 is a 8 bit un-signed integer , it means it can store only positive values
Range->> Integer values from (0 to 255) ie [0 to 2^8 -1]
uint8 has a length of 8 bits (1 bytes).

uint16 is a 16 bit un-signed integer , it means it can store only positive values
Range->> Integer values from (0 to 65535) ie [0 to 2^16 -1]
uint16 has a length of 16 bits (2 bytes).


uint32 is a 32 bit un-signed integer , it means it can store only positive values
Range->> Integer values from (0 to 4294967295) ie [0 to 2^32 -1]
uint32 has a length of 32 bits (4 bytes).


uint64 is a 64 bit un-signed integer , it means it can store only positive values
Range->> Integer values from (0 to 18446744073709551615) ie [0 to 2^64 -1]
uint64 has a length of 64 bits (8 bytes).

'''

"""# 3.. Which month have max. Uber pickups in New York City ?"""

uber_15

uber_15['month'] = uber_15['Pickup_date'].dt.month_name()

uber_15['month']

uber_15['month'].value_counts().plot()

uber_15['month'].value_counts().plot(kind='bar')

'''
Inference : June seems to have max Uber Pickups

'''

## extracting dervied features (weekday ,day ,hour ,month ,minute) from 'Pickup_date'..

uber_15['weekday'] = uber_15['Pickup_date'].dt.day_name()
uber_15['day'] = uber_15['Pickup_date'].dt.day
uber_15['hour'] = uber_15['Pickup_date'].dt.hour
uber_15['minute'] = uber_15['Pickup_date'].dt.minute

uber_15.head(4)

## pd.crosstab() is used to create pivot table ..

pivot = pd.crosstab(index=uber_15['month'] , columns=uber_15['weekday'])

pivot

## grouped-bar plot using Pandas ..
pivot.plot(kind='bar' , figsize=(8,6))

'''

On Saturday & Friday, u are getting more Uber pickups in each month , it seems that New Yorkers used to go for
shopping , Malls , fun activities alot on these days

'''

"""# 4.. Lets Find out Hourly Rush in New york city on all days"""

summary = uber_15.groupby(['weekday' , 'hour'] , as_index=False).size()

summary

## pointplot between 'hour' & 'size' for all the weekdays..

plt.figure(figsize=(8,6))
sns.pointplot(x="hour" , y="size" , hue="weekday" , data=summary)

'''
It's interesting to see that Saturday and Sunday exhibit similar demand throughout the late night/morning/afternoon,
but it exhibits opposite trends during the evening. In the evening, Saturday pickups continue to increase throughout the evening,
but Sunday pickups takes a downward turn after evening..

We can see that there the weekdays that has the most demand during the late evening is Friday and Saturday,
which is expected, but what strikes me is that Thursday nights also exhibits very similar trends as Friday and Saturday nights.

It seems like New Yorkers are starting their 'weekends' on Thursday nights.


'''

"""# 5.. Which Base_number has most number of Active Vehicles ??"""

uber_15.columns

import os

files = os.listdir("uber_data/Datasets")
print(files)

files = os.listdir("uber_data/Datasets")
print(files)

uber_foil = pd.read_csv(r"uber_data/Datasets/Uber-Jan-Feb-FOIL.csv")

uber_foil.shape

uber_foil.head(3)

### establishing the entire set-up of Plotly..

!pip install chart_studio ## chart_studio provides a web-service for hosting graphs!
!pip install plotly

import chart_studio.plotly as py
import plotly.graph_objs as go
import plotly.express as px

import pandas as pd
import plotly.express as px
import plotly.io as pio

from plotly.offline import download_plotlyjs , init_notebook_mode , plot , iplot

pio.renderers.default = 'colab'

init_notebook_mode(connected=True)

uber_foil.columns

fig = px.box(x='dispatching_base_number' , y='active_vehicles' , data_frame=uber_foil)
fig.show()

px.violin(x='dispatching_base_number' , y='active_vehicles' , data_frame=uber_foil)

"""# 6.. Collect entire data & Make it ready for the Data Analysis.."""

import os

files = os.listdir("uber_data/Datasets")
print(files)

files

files

files.remove('uber-raw-data-janjune-15_sample.csv')

files

#blank dataframe
final = pd.DataFrame()

path = r"uber_data/Datasets"

for file in files :
    final = pd.concat([current_df , final])

final.shape

### After Collecting entire data ,u might ask is : Do we have duplicate entires in data ?
### We are going to remove duplicates data when the entire row is duplicated

### first lets figure out total observations where we have duplicate values..
final.duplicated().sum()

## drop duplicate rows ..
final.drop_duplicates(inplace=True)

final.shape

final.head(3)

"""## Dataset Information :

# 7.. at what locations of New York City we are getting rush ??
"""

### ie where-ever we have more data-points or more density, it means more rush is at there !

rush_uber = final.groupby(['Lat' , 'Lon'] , as_index=False).size()

rush_uber.head(6)

!pip install folium

import folium

basemap = folium.Map()

basemap

from folium.plugins import HeatMap

HeatMap(rush_uber).add_to(basemap)

basemap

"""We can see a number of hot spots here. Midtown Manhattan is clearly a huge bright spot
    & these are made from Midtown to Lower Manhattan followed by Upper Manhattan and the Heights of Brooklyn.

# 8.. Examine rush on Hour and Weekday ( Perform Pair wise Analysis )
"""

final.columns

final.head(3)

final.dtypes

final['Date/Time'][0]

### converting 'Date/Time' feature into date-time..

final['Date/Time'] = pd.to_datetime(final['Date/Time'] , format="%m/%d/%Y %H:%M:%S")

final['Date/Time'].dtype

### extracting 'weekday' & 'hour' from 'Date/Time' feature..

final['day'] = final['Date/Time'].dt.day
final['hour'] = final['Date/Time'].dt.hour

final.head(4)

'''
Earlier we have learned how to create pivot table using pd.crosstab() , now let me show you one more way to build
pivot_table without pd.crosstab()

'''

pivot = final.groupby(['day' , 'hour']).size().unstack()

pivot

### pivot table is all about  , we have Rows*columns & having value in each cell !

### styling dataframe

pivot.style.background_gradient()

"""# 9.. How to Automate Your Analysis..?"""

## creating a user-defined function..

def gen_pivot_table(df , col1 , col2):
    pivot = final.groupby([col1 , col2]).size().unstack()
    return pivot.style.background_gradient()

final.columns

gen_pivot_table(final , "day" , "hour")